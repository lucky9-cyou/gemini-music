import streamlit as st
import google.generativeai as genai
import os
import time
import asyncio
import threading

# --- Streamlit é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="ğŸµ Gemini éŸ³é¢‘æ™ºèƒ½åŠ©æ‰‹ - å¹¶å‘ç‰ˆ",
    page_icon="âœ¨",
    layout="wide", # å®½å¸ƒå±€
    initial_sidebar_state="expanded" # ä¾§è¾¹æ é»˜è®¤å±•å¼€
)

# --- é¡¶éƒ¨æ ‡é¢˜å’Œæè¿° ---
st.markdown("<h1 style='text-align: center; color: #4CAF50;'>ğŸµ Gemini éŸ³é¢‘æ™ºèƒ½åŠ©æ‰‹ âœ¨ (å¹¶å‘æ¨¡å¼)</h1>", unsafe_allow_html=True)
st.markdown("<p style='text-align: center; color: grey;'>åˆ©ç”¨ Google Gemini å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ·±åº¦åˆ†æå’Œç†è§£æ‚¨çš„éŸ³é¢‘å†…å®¹ï¼Œå¹¶å¯åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚ã€‚</p>", unsafe_allow_html=True)
st.divider() # åˆ†å‰²çº¿ï¼Œè®©å¸ƒå±€æ›´æ¸…æ™°

# --- Google API Key é…ç½® ---
api_key = os.getenv("GOOGLE_API_KEY") # ç›´æ¥ä»ç¯å¢ƒå˜é‡è·å–

# API Key æç¤ºä¸é…ç½®å¼•å¯¼
if not api_key:
    st.warning("âš ï¸ **Google API Key æœªè®¾ç½®ï¼** è¯·åœ¨ç»§ç»­ä¹‹å‰é…ç½®ã€‚", icon="ğŸ”‘")
    st.info("""
    **é…ç½®æ–¹å¼ï¼š**
    - **éƒ¨ç½²åˆ° Zeabur**: åœ¨ Zeabur æ§åˆ¶å°çš„ç¯å¢ƒå˜é‡ä¸­æ·»åŠ  `GOOGLE_API_KEY = "your_api_key_here"`
    - **æœ¬åœ°è¿è¡Œ**: è®¾ç½®ç¯å¢ƒå˜é‡ `export GOOGLE_API_KEY="your_api_key_here"`
    """)
    st.stop() # å¦‚æœæ²¡æœ‰ API Keyï¼Œåœæ­¢åº”ç”¨è¿è¡Œ

try:
    genai.configure(api_key=api_key)
except Exception as e:
    st.error(f"âŒ é…ç½® Google GenAI å¤±è´¥ï¼š{e}", icon="â›”")
    st.stop()

# --- Streamlit Session State åˆå§‹åŒ– ---
if 'tasks' not in st.session_state:
    st.session_state.tasks = [] # å­˜å‚¨æ‰€æœ‰å¹¶å‘ä»»åŠ¡çš„çŠ¶æ€å’Œç»“æœ
if 'running_concurrently' not in st.session_state:
    st.session_state.running_concurrently = False # æ ‡è®°æ˜¯å¦æœ‰å¹¶å‘ä»»åŠ¡æ­£åœ¨è¿è¡Œ

# --- å®šä¹‰å¼‚æ­¥ä»»åŠ¡å‡½æ•° ---
# è¿™æ˜¯ä¸€ä¸ªåœ¨åå°å¼‚æ­¥æ‰§è¡Œçš„å•ä¸ª Gemini API è°ƒç”¨
async def async_gemini_analysis_task(task_index, prompt, audio_bytes, mime_type, model_id):
    """
    å¼‚æ­¥æ‰§è¡Œå•ä¸ª Gemini éŸ³é¢‘åˆ†æä»»åŠ¡ã€‚
    æ›´æ–° session_state å¹¶åœ¨æ¯æ¬¡çŠ¶æ€å˜åŒ–åè°ƒç”¨ st.rerun()ã€‚
    """
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºâ€œè¿›è¡Œä¸­â€
        st.session_state.tasks[task_index]['status'] = 'è¿›è¡Œä¸­'
        st.session_state.tasks[task_index]['message'] = f"æ­£åœ¨ç”¨ {model_id} æ¨¡å‹åˆ†æ..."
        st.rerun() # é€šçŸ¥ Streamlit åˆ·æ–° UI

        model = genai.GenerativeModel(model_id)
        
        # å®é™…çš„ Gemini API è°ƒç”¨
        # æ³¨æ„ï¼šgenai.generate_content æœ¬èº«ä¸æ˜¯ asyncï¼Œä½†åœ¨ asyncio çº¿ç¨‹ä¸­è¿è¡Œæ²¡é—®é¢˜
        response = model.generate_content(
            contents=[prompt, (audio_bytes, mime_type)]
        )

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºâ€œå®Œæˆâ€
        st.session_state.tasks[task_index]['status'] = 'å®Œæˆ'
        st.session_state.tasks[task_index]['result'] = response.text
        st.session_state.tasks[task_index]['message'] = 'åˆ†ææˆåŠŸï¼'
        st.rerun() # é€šçŸ¥ Streamlit åˆ·æ–° UI

    except genai.types.BlockedPromptException as e:
        st.session_state.tasks[task_index]['status'] = 'è¢«é˜»æ­¢'
        st.session_state.tasks[task_index]['message'] = f"è¯·æ±‚è¢«æ¨¡å‹å®‰å…¨è®¾ç½®é˜»æ­¢: {e}"
        st.session_state.tasks[task_index]['error_details'] = str(e)
        st.rerun()
    except Exception as e:
        st.session_state.tasks[task_index]['status'] = 'å¤±è´¥'
        st.session_state.tasks[task_index]['message'] = f"åˆ†æå¤±è´¥: {e}"
        st.session_state.tasks[task_index]['error_details'] = str(e)
        st.rerun()


# --- åå°çº¿ç¨‹å‡½æ•° ---
def run_concurrent_analysis_thread(num_concurrent_requests, user_prompt, uploaded_file_bytes, uploaded_file_type, model_id):
    """
    åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œ asyncio äº‹ä»¶å¾ªç¯ï¼Œæ‰§è¡Œå¹¶å‘ä»»åŠ¡ã€‚
    """
    st.session_state.running_concurrently = True
    st.session_state.tasks = [] # é‡ç½®ä»»åŠ¡åˆ—è¡¨

    # åˆå§‹åŒ–æ‰€æœ‰ä»»åŠ¡çš„çŠ¶æ€
    for i in range(num_concurrent_requests):
        st.session_state.tasks.append({
            'id': i + 1,
            'status': 'å¾…å¤„ç†',
            'message': 'ç­‰å¾…å¼€å§‹...',
            'result': None,
            'error_details': None
        })
    st.rerun() # ç«‹å³åˆ·æ–° UIï¼Œæ˜¾ç¤ºæ‰€æœ‰å¾…å¤„ç†ä»»åŠ¡

    # åˆ›å»ºä¸€ä¸ª asyncio äº‹ä»¶å¾ªç¯å¹¶åœ¨å…¶ä¸­è¿è¡Œå¹¶å‘ä»»åŠ¡
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    tasks_to_run = [
        async_gemini_analysis_task(
            i,
            user_prompt,
            uploaded_file_bytes,
            uploaded_file_type,
            model_id
        )
        for i in range(num_concurrent_requests)
    ]

    # ç­‰å¾…æ‰€æœ‰å¹¶å‘ä»»åŠ¡å®Œæˆ
    loop.run_until_complete(asyncio.gather(*tasks_to_run))
    loop.close()

    st.session_state.running_concurrently = False
    st.rerun() # æ‰€æœ‰ä»»åŠ¡å®Œæˆåï¼Œå†æ¬¡åˆ·æ–° UI


# --- ç”¨æˆ·ç•Œé¢ (ä¾§è¾¹æ ) ---
st.sidebar.header("âš™ï¸ æ“ä½œé¢æ¿")

# æ¨¡å‹é€‰æ‹©
st.sidebar.subheader("ğŸ¤– é€‰æ‹© Gemini æ¨¡å‹")
model_options = {
    "Gemini 2.5 Flash (å¿«é€Ÿé«˜æ•ˆ)": "gemini-2.5-flash",
    "Gemini 2.5 Pro (æ·±åº¦ç†è§£)": "gemini-2.5-pro",
}
selected_model_name = st.sidebar.selectbox(
    "æ¨¡å‹ç‰ˆæœ¬:",
    options=list(model_options.keys()),
    index=0, # é»˜è®¤é€‰ä¸­ç¬¬ä¸€ä¸ª
    help="Gemini 2.5 Flash é€Ÿåº¦æ›´å¿«ã€æˆæœ¬æ›´ä½ï¼Œé€‚åˆå¿«é€Ÿæ¦‚æ‹¬ï¼›Gemini 2.5 Pro åŠŸèƒ½æ›´å¼ºå¤§ï¼Œç†è§£æ›´æ·±å…¥ï¼Œä½†å¯èƒ½å“åº”ç¨æ…¢ä¸”æˆæœ¬æ›´é«˜ã€‚"
)
selected_model_id = model_options[selected_model_name]

st.sidebar.divider() # åˆ†å‰²çº¿

# éŸ³é¢‘æ–‡ä»¶ä¸Šä¼ 
st.sidebar.subheader("ğŸ“¤ ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶")
uploaded_file = st.sidebar.file_uploader(
    "é€‰æ‹©ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ (.mp3, .wav, .flac ç­‰)",
    type=["mp3", "wav", "flac", "ogg", "m4a"],
    help="æ”¯æŒçš„éŸ³é¢‘æ ¼å¼åŒ…æ‹¬ MP3ã€WAVã€FLACã€OGGã€M4A ç­‰ã€‚è¯·æ³¨æ„ï¼Œç›´æ¥å†…åµŒéŸ³é¢‘åˆ°è¯·æ±‚ä¸­é€šå¸¸é€‚åˆ**è¾ƒçŸ­**çš„éŸ³é¢‘ç‰‡æ®µï¼ˆå»ºè®®å‡ MBä»¥å†…ï¼Œä¾‹å¦‚1-4åˆ†é’Ÿï¼‰ï¼Œè¿‡å¤§æ–‡ä»¶å¯èƒ½å¯¼è‡´è¯·æ±‚å¤±è´¥ã€‚"
)

st.sidebar.divider() # åˆ†å‰²çº¿

# Prompt è¾“å…¥
st.sidebar.subheader("ğŸ“ å®šä¹‰æ‚¨çš„åˆ†æä»»åŠ¡")
default_prompt = "è¯·è¯¦ç»†æè¿°è¿™ä¸ªéŸ³é¢‘å‰ªè¾‘çš„å†…å®¹ï¼Œè¯†åˆ«å…¶ä¸­çš„ä»»ä½•å£°éŸ³ã€éŸ³ä¹æˆ–è¯­éŸ³ã€‚æ€»ç»“å…¶ä¸»è¦ä¿¡æ¯ã€‚å¦‚æœåŒ…å«è¯­éŸ³ï¼Œè¯·å°è¯•è½¬å½•å…³é”®ä¿¡æ¯ã€‚"
user_prompt = st.sidebar.text_area(
    "å¯¹æ¨¡å‹è¯´äº›ä»€ä¹ˆï¼Ÿ",
    value=default_prompt,
    height=300, # å¢å¤§é«˜åº¦åˆ° 300
    help="è¾“å…¥æ‚¨æƒ³è®© Gemini æ¨¡å‹å¯¹éŸ³é¢‘æ‰§è¡Œçš„ä»»åŠ¡æˆ–æé—®ã€‚ä¾‹å¦‚ï¼š'åˆ†æè¿™æ®µéŸ³ä¹çš„é£æ ¼å’Œæƒ…ç»ª'ã€'è½¬å½•è¿™æ®µå¯¹è¯çš„ä¸»è¦å†…å®¹'ç­‰ã€‚"
)

st.sidebar.divider()

# å¹¶å‘è¯·æ±‚æ•°é‡è®¾ç½®
num_concurrent_requests = st.sidebar.slider(
    "å¹¶å‘è¯·æ±‚æ•°é‡:",
    min_value=1,
    max_value=10, # æœ€å¤§è®¾ç½®ä¸º 10 ä¸ªå¹¶å‘
    value=5, # é»˜è®¤ 5 ä¸ª
    help="åŒæ—¶å‘ Gemini API å‘é€çš„è¯·æ±‚æ•°é‡ã€‚è¿‡é«˜çš„å¹¶å‘æ•°å¯èƒ½è§¦å‘ API é€Ÿç‡é™åˆ¶ã€‚"
)

st.sidebar.markdown("---")

# æŒ‰é’®ï¼šå•æ¬¡åˆ†æ å’Œ å¹¶å‘åˆ†æ
col_btn1, col_btn2 = st.sidebar.columns(2)

with col_btn1:
    single_analyze_button = st.button("ğŸš€ å•æ¬¡åˆ†æ", use_container_width=True, disabled=st.session_state.running_concurrently)
with col_btn2:
    concurrent_analyze_button = st.button("âš¡ å¹¶å‘åˆ†æ", use_container_width=True, disabled=st.session_state.running_concurrently)

# --- ä¸»å†…å®¹åŒºåŸŸ ---
main_output_container = st.container()

with main_output_container:
    status_message_area = st.empty()
    uploaded_audio_preview_expander = st.expander("â–¶ï¸ ç‚¹å‡»é¢„è§ˆå·²ä¸Šä¼ éŸ³é¢‘", expanded=False)
    
    if st.session_state.running_concurrently:
        st.info("âš¡ï¸ å¹¶å‘ä»»åŠ¡æ­£åœ¨åå°è¿è¡Œï¼Œè¯·ç­‰å¾…...")
        # éå†æ˜¾ç¤ºæ‰€æœ‰å¹¶å‘ä»»åŠ¡çš„å®æ—¶çŠ¶æ€
        st.subheader(f"ğŸ“Š å¹¶å‘ä»»åŠ¡ ({len(st.session_state.tasks)}/{num_concurrent_requests}) çŠ¶æ€:")
        for task in st.session_state.tasks:
            status_emoji = {
                'å¾…å¤„ç†': 'âšª',
                'è¿›è¡Œä¸­': 'â³',
                'å®Œæˆ': 'âœ…',
                'å¤±è´¥': 'âŒ',
                'è¢«é˜»æ­¢': 'ğŸš«'
            }.get(task['status'], 'â“')
            
            with st.status(f"ä»»åŠ¡ {task['id']}: {status_emoji} {task['message']}", expanded=(task['status'] in ['å¤±è´¥', 'è¢«é˜»æ­¢'])) as task_status_container:
                if task['result']:
                    st.markdown("**åˆ†æç»“æœ:**")
                    st.markdown(task['result'])
                if task['error_details']:
                    st.error(f"**é”™è¯¯è¯¦æƒ…:**\n```\n{task['error_details']}\n```")
            st.markdown("---") # æ¯ä¸ªä»»åŠ¡ä¹‹é—´åŠ ä¸ªåˆ†éš”çº¿

    else: # å¦‚æœæ²¡æœ‰å¹¶å‘ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œæ˜¾ç¤ºå•ä¸ªä»»åŠ¡çš„ç»“æœ
        analysis_result_expander = st.expander("âœ¨ Gemini åˆ†æç»“æœ", expanded=False)


# --- å¤„ç†é€»è¾‘ (å•æ¬¡åˆ†æ) ---
if single_analyze_button and not st.session_state.running_concurrently:
    if uploaded_file is None:
        status_message_area.warning("âš ï¸ è¯·å…ˆä¸Šä¼ ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼", icon="â¬†ï¸")
        uploaded_audio_preview_expander.empty()
        analysis_result_expander.empty()
    else:
        status_message_area.empty()
        uploaded_audio_preview_expander.empty()
        analysis_result_expander.empty()
        
        uploaded_audio_preview_expander.expanded = True
        analysis_result_expander.expanded = True

        status_message_area.info("â³ æ­£åœ¨å‡†å¤‡åˆ†æï¼Œè¯·ç¨å€™...", icon="ğŸ”„")

        try:
            with uploaded_audio_preview_expander:
                st.subheader("å·²ä¸Šä¼ éŸ³é¢‘:")
                st.audio(uploaded_file, format=uploaded_file.type, start_time=0)
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown(f"**æ–‡ä»¶åç§°:** `{uploaded_file.name}`")
                with col2:
                    st.markdown(f"**æ–‡ä»¶å¤§å°:** `{round(uploaded_file.size / (1024 * 1024), 2)} MB`")
                st.markdown("---")


            with status_message_area.status("æ­£åœ¨å‡†å¤‡éŸ³é¢‘å†…å®¹...", expanded=True) as status_box:
                audio_bytes = uploaded_file.getvalue()
                status_box.update(label="éŸ³é¢‘å†…å®¹å·²å‡†å¤‡å°±ç»ªï¼", state="complete", expanded=False)


            with status_message_area.status(f"æ­£åœ¨ä½¿ç”¨ `{selected_model_name}` æ¨¡å‹åˆ†æå†…å®¹...", expanded=True) as status_box:
                model = genai.GenerativeModel(selected_model_id)
                response = model.generate_content(
                    contents=[user_prompt, (audio_bytes, uploaded_file.type)]
                )
                status_box.update(label="æ¨¡å‹å“åº”å·²è·å–ï¼", state="complete", expanded=False)


            status_message_area.success("âœ… åˆ†æå®Œæˆï¼è¯·æŸ¥çœ‹ä¸‹æ–¹ç»“æœã€‚", icon="ğŸ‰")
            with analysis_result_expander:
                st.markdown(f"### ğŸ¤– Gemini çš„è¯¦ç»†åˆ†æç»“æœ ({selected_model_name}):")
                st.markdown(response.text)


        except genai.types.BlockedPromptException as e:
            status_message_area.error(f"âš ï¸ æ‚¨çš„è¯·æ±‚è¢«æ¨¡å‹å®‰å…¨è®¾ç½®é˜»æ­¢äº†ã€‚è¯·å°è¯•ä¿®æ”¹ Prompt æˆ–éŸ³é¢‘å†…å®¹ã€‚", icon="ğŸš«")
            st.exception(e)
            uploaded_audio_preview_expander.empty()
            analysis_result_expander.empty()
        except Exception as e:
            status_message_area.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯ã€‚", icon="â›”")
            status_message_area.warning("ğŸ’¡ è¯·æ£€æŸ¥æ‚¨çš„ API Key æ˜¯å¦æœ‰æ•ˆã€ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ï¼Œä»¥åŠä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆä¸”ç¬¦åˆæ¨¡å‹å¤„ç†è¦æ±‚ï¼ˆä¾‹å¦‚ï¼ŒéŸ³é¢‘å†…å®¹æ˜¯å¦æ¸…æ™°å¯è¯†åˆ«ï¼Œæ–‡ä»¶å¤§å°æ˜¯å¦è¿‡å¤§ï¼Œæ—¶é•¿æ˜¯å¦è¶…è¿‡10åˆ†é’Ÿï¼‰ã€‚", icon="ğŸ”")
            st.exception(e)
            uploaded_audio_preview_expander.empty()
            analysis_result_expander.empty()
        finally:
            pass


# --- å¤„ç†é€»è¾‘ (å¹¶å‘åˆ†æ) ---
if concurrent_analyze_button and not st.session_state.running_concurrently:
    if uploaded_file is None:
        status_message_area.warning("âš ï¸ è¯·å…ˆä¸Šä¼ ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼", icon="â¬†ï¸")
    else:
        # è·å–ä¸Šä¼ æ–‡ä»¶çš„å­—èŠ‚æµå’ŒMIMEç±»å‹ï¼Œç”¨äºä¼ é€’ç»™å¹¶å‘ä»»åŠ¡
        uploaded_file_bytes = uploaded_file.getvalue()
        uploaded_file_type = uploaded_file.type

        status_message_area.info(f"âš¡ï¸ æ­£åœ¨å¯åŠ¨ {num_concurrent_requests} ä¸ªå¹¶å‘åˆ†æä»»åŠ¡...", icon="ğŸš€")
        
        # å¯åŠ¨ä¸€ä¸ªæ–°çº¿ç¨‹æ¥è¿è¡Œå¹¶å‘åˆ†æ
        thread = threading.Thread(
            target=run_concurrent_analysis_thread,
            args=(num_concurrent_requests, user_prompt, uploaded_file_bytes, uploaded_file_type, selected_model_id)
        )
        thread.start()
        # Streamlit è„šæœ¬ä¼šç«‹å³é‡æ–°è¿è¡Œå¹¶æ˜¾ç¤ºåˆå§‹ä»»åŠ¡çŠ¶æ€


# --- é¡µè„š (å¯é€‰) ---
st.markdown("---")
st.markdown("""
    <p style='text-align: center; color: grey; font-size: 0.9em;'>
        ç”± Streamlit & Google Gemini API é©±åŠ¨ ğŸš€ <br>
        å¦‚æœæ‚¨å–œæ¬¢è¿™ä¸ªåº”ç”¨ï¼Œè¯·åˆ†äº«ç»™æ‚¨çš„æœ‹å‹ï¼
    </p>
""", unsafe_allow_html=True)